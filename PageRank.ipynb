{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total length of a file : reduce_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"test1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark Examples',\n",
       " 'These examples give a quick overview of the Spark API.',\n",
       " 'Spark is built on the concept of distributed datasets, which contain arbitrary Java or Python objects. You create a dataset from external data, then apply parallel operations to it. The building block of the Spark API is its RDD API. In the RDD API, there are two types of operations: transformations, which define a new dataset based on previous ones, and actions, which kick off a job to execute on a cluster. On top of Sparkâ€™s RDD API, high level APIs are provided, e.g. DataFrame API and Machine Learning API. These high level APIs provide a concise way to conduct certain data operations. In this page, we will show examples using RDD API as well as examples using high level APIs.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.count()  # number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineLengths = text_file.map(lambda s: len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 55, 686]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "totalLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 66), ('b', 31)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize([['a', 21],['b',31], ['a',22], ['a',23]])\n",
    "data1.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"test2.txt\")\n",
    "lines = text_file.filter(lambda line : \"line\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a first line', 'This is a second line', 'This is the last line']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the last line']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text_file.filter(lambda line : \"last\" in line)\n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the last line'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carlo,5,3,3,4',\n",
       " 'Mokhtar,2,5,5,3',\n",
       " 'Jacques,4,2,4,5',\n",
       " 'Braden,5,3,2,5',\n",
       " 'Chris,5,4,5,1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"test3.txt\")\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Carlo', '5', '3', '3', '4'],\n",
       " ['Mokhtar', '2', '5', '5', '3'],\n",
       " ['Jacques', '4', '2', '4', '5'],\n",
       " ['Braden', '5', '3', '2', '5'],\n",
       " ['Chris', '5', '4', '5', '1']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data.map(lambda l: l.split(\",\"))\n",
    "data1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carlo', '5334'),\n",
       " ('Mokhtar', '2553'),\n",
       " ('Jacques', '4245'),\n",
       " ('Braden', '5325'),\n",
       " ('Chris', '5451')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data1.map(lambda item: (item[0], item[1]+item[2]+item[3]+item[4]))\n",
    "data2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carlo', 15),\n",
       " ('Mokhtar', 15),\n",
       " ('Jacques', 15),\n",
       " ('Braden', 15),\n",
       " ('Chris', 15)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data1.map(lambda item: (item[0], int(item[1])+int(item[2])+int(item[3])+int(item[4])))\n",
    "data2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Carlo', 15, 3.75),\n",
       " ('Mokhtar', 15, 3.75),\n",
       " ('Jacques', 15, 3.75),\n",
       " ('Braden', 15, 3.75),\n",
       " ('Chris', 15, 3.75)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3 = data2.map(lambda item: (item[0], item[1], item[1]/4))\n",
    "data3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['maths', 50], ['maths', 60], ['english', 65], ['english', 85]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputrdd = sc.parallelize([ [\"maths\", 50], [\"maths\", 60], [\"english\", 65],  [\"english\", 85]])\n",
    "inputrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maths', (50, 1)),\n",
       " ('maths', (60, 1)),\n",
       " ('english', (65, 1)),\n",
       " ('english', (85, 1))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped = inputrdd.mapValues(lambda mark : (mark, 1))\n",
    "mapped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('english', ((65, 1), (85, 1))), ('maths', ((50, 1), (60, 1)))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped.reduceByKey(lambda x,y : (x,y)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('english', (150, 2)), ('maths', (110, 2))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced = mapped.reduceByKey(lambda x, y : (x[0]+ y[0] , x[1]+ y[1]))\n",
    "reduced.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('english', 75.0), ('maths', 55.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average = reduced.map(lambda  x : (x[0], x[1][0]/x[1][1]))\n",
    "average.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Rank\n",
    "http://www.openkb.info/2016/03/understanding-pagerank-algorithm-in.html\n",
    "https://eyeballs.tistory.com/70\n",
    "<img src=\"pagerank1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapLink = sc.parallelize([ [\"MapR\",\"Baidu\"],[\"MapR\", \"Blogger\"],[\"Baidu\", \"MapR\"], \\\n",
    "                         [\"Blogger\",\"Google\"], [\"Blogger\", \"Baidu\"],[\"Google\", \"MapR\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = mapLink.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', <pyspark.resultiterable.ResultIterable at 0x217efcc8a58>),\n",
       " ('MapR', <pyspark.resultiterable.ResultIterable at 0x217efcc8a20>),\n",
       " ('Baidu', <pyspark.resultiterable.ResultIterable at 0x217efcc8b00>),\n",
       " ('Blogger', <pyspark.resultiterable.ResultIterable at 0x217efcc8be0>)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n",
      "['MapR']\n",
      "MapR\n",
      "['Baidu', 'Blogger']\n",
      "Baidu\n",
      "['MapR']\n",
      "Blogger\n",
      "['Google', 'Baidu']\n"
     ]
    }
   ],
   "source": [
    "for k,v in links.collect():\n",
    "    print(k),\n",
    "    print(list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', ['MapR']), ('MapR', ['Baidu', 'Blogger']), ('Baidu', ['MapR']), ('Blogger', ['Google', 'Baidu'])]\n"
     ]
    }
   ],
   "source": [
    "print(list((k, list(v)) for (k, v) in links.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 1), ('MapR', 1), ('Baidu', 1), ('Blogger', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = links.map(lambda pairs : (pairs[0],1))\n",
    "ranks.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kim', (2222, 'ccc@eeee')), ('park', (1122, 'abc@dddd'))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize([['park', 1122],['kim', 2222]])\n",
    "data2 = sc.parallelize([['park', 'abc@dddd'], ['kim','ccc@eeee'], ['choi', 'www@ffff']])\n",
    "data1.join(data2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', ('choi', 3)), ('a', (1, ['kim', 'c']))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = sc.parallelize([['a',1,'kim'],['b','choi',2]])\n",
    "data2 = sc.parallelize([['a',['kim','c'],'choi'], ['b',3,4]])\n",
    "data1.join(data2).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', (<pyspark.resultiterable.ResultIterable at 0x217efcaaa58>, 1)),\n",
       " ('MapR', (<pyspark.resultiterable.ResultIterable at 0x217efcaa550>, 1)),\n",
       " ('Baidu', (<pyspark.resultiterable.ResultIterable at 0x217efcaa6a0>, 1)),\n",
       " ('Blogger', (<pyspark.resultiterable.ResultIterable at 0x217efcaaf28>, 1))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvalues  = links.join(ranks)\n",
    "cvalues.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', [<pyspark.resultiterable.ResultIterable object at 0x00000217EFCDA1D0>, 1]), ('MapR', [<pyspark.resultiterable.ResultIterable object at 0x00000217EFCDA9B0>, 1]), ('Baidu', [<pyspark.resultiterable.ResultIterable object at 0x00000217EFCDA278>, 1]), ('Blogger', [<pyspark.resultiterable.ResultIterable object at 0x00000217EFCDAE80>, 1])]\n",
      "[('Google', (['MapR'], 1)), ('MapR', (['Baidu', 'Blogger'], 1)), ('Baidu', (['MapR'], 1)), ('Blogger', (['Google', 'Baidu'], 1))]\n"
     ]
    }
   ],
   "source": [
    "cvalues  = links.join(ranks)\n",
    "\"\"\"\n",
    "for (k,v) in cvalues.collect():\n",
    "    print(k),\n",
    "    print(list(v[0])),\n",
    "    print(v[1])\n",
    "\"\"\"\n",
    "print(list((k,  list(v)) for (k,v) in cvalues.collect()))\n",
    "print(list((k,  (list(v[0]), v[1])) for (k,v) in cvalues.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pagerank2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Baidu', 0.5)\n",
      "('Blogger', 0.5)\n"
     ]
    }
   ],
   "source": [
    "a = computeContribs(['Baidu', 'Blogger'], 1)\n",
    "for i in a :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.3333333333333333)\n",
      "(2, 0.3333333333333333)\n",
      "(3, 0.3333333333333333)\n"
     ]
    }
   ],
   "source": [
    "a = computeContribs([1,2,3],1)\n",
    "for i in a :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Google',\n",
       " (<pyspark.resultiterable.ResultIterable at 0x217efcda898>, 1),\n",
       " 'MapR',\n",
       " (<pyspark.resultiterable.ResultIterable at 0x217efcda278>, 1),\n",
       " 'Baidu',\n",
       " (<pyspark.resultiterable.ResultIterable at 0x217efcda5c0>, 1),\n",
       " 'Blogger',\n",
       " (<pyspark.resultiterable.ResultIterable at 0x217efcda710>, 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links.join(ranks).flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapR', 1.0),\n",
       " ('Baidu', 0.5),\n",
       " ('Blogger', 0.5),\n",
       " ('MapR', 1.0),\n",
       " ('Google', 0.5),\n",
       " ('Baidu', 0.5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "        \n",
    "contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "contribs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[('Google', (['MapR'], 1)), ('MapR', (['Baidu', 'Blogger'], 1)), ('Baidu', (['MapR'], 1)), ('Blogger', (['Google', 'Baidu'], 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('MapR', 1.0)],\n",
       " [('Baidu', 0.5), ('Blogger', 0.5)],\n",
       " [('MapR', 1.0)],\n",
       " [('Google', 0.5), ('Baidu', 0.5)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeContribs1(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        result.append( (url, rank / num_urls))\n",
    "    return result\n",
    "        \n",
    "contribs1 = links.join(ranks).map(\n",
    "            lambda url_urls_rank: computeContribs1(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "contribs1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MapR', 1.0),\n",
       " ('Baidu', 0.5),\n",
       " ('Blogger', 0.5),\n",
       " ('MapR', 1.0),\n",
       " ('Google', 0.5),\n",
       " ('Baidu', 0.5)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeContribs1(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    result = []\n",
    "    for url in urls:\n",
    "        result.append( (url, rank / num_urls))\n",
    "    return result\n",
    "        \n",
    "contribs1 = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs1(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "contribs1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 0.5), ('MapR', 2.0), ('Baidu', 1.0), ('Blogger', 0.5)]\n",
      "[('Google', 0.575), ('MapR', 1.8499999999999999), ('Baidu', 1.0), ('Blogger', 0.575)]\n"
     ]
    }
   ],
   "source": [
    "new_rank = contribs.reduceByKey(lambda x, y : x+y).collect()\n",
    "print(new_rank)\n",
    "new_rank1 = contribs.reduceByKey(lambda x, y : x+y).mapValues(lambda rank:0.15+0.85*rank).collect()\n",
    "print(new_rank1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Google', 1), ('MapR', 1), ('Baidu', 1), ('Blogger', 1)]\n",
      "[('Google', 0.575), ('MapR', 1.8499999999999999), ('Baidu', 1.0), ('Blogger', 0.575)]\n"
     ]
    }
   ],
   "source": [
    "mapLink = sc.parallelize([ [\"MapR\",\"Baidu\"],[\"MapR\", \"Blogger\"],[\"Baidu\", \"MapR\"], \\\n",
    "                         [\"Blogger\",\"Google\"], [\"Blogger\", \"Baidu\"],[\"Google\", \"MapR\"]])\n",
    "links = mapLink.groupByKey()\n",
    "\n",
    "ranks = links.map(lambda pairs : (pairs[0],1))\n",
    "\n",
    "cvalues  = links.join(ranks)\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "        \n",
    "contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "\n",
    "new_rank = contribs.reduceByKey(lambda x, y : x+y).mapValues(lambda rank:0.15+0.85*rank).collect()\n",
    "print(ranks.collect())\n",
    "print(new_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial ranks: [('Google', 1), ('MapR', 1), ('Baidu', 1), ('Blogger', 1)]\n",
      "ranks( 0 ): [('Google', 0.575), ('MapR', 1.8499999999999999), ('Baidu', 1.0), ('Blogger', 0.575)]\n",
      "\n",
      "ranks( 1 ): [('MapR', 1.4887499999999998), ('Blogger', 0.9362499999999999), ('Baidu', 1.1806249999999998), ('Google', 0.394375)]\n",
      "\n",
      "ranks( 2 ): [('Google', 0.5479062499999999), ('MapR', 1.4887499999999996), ('Baidu', 1.1806249999999998), ('Blogger', 0.7827187499999999)]\n",
      "\n",
      "ranks( 3 ): [('MapR', 1.6192515624999995), ('Baidu', 1.1153742187499998), ('Blogger', 0.7827187499999998), ('Google', 0.48265546875)]\n",
      "\n",
      "ranks( 4 ): [('MapR', 1.5083252343749995), ('Google', 0.48265546874999987), ('Blogger', 0.8381819140624998), ('Baidu', 1.1708373828124996)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "        \n",
    "mapLink = sc.parallelize([ [\"MapR\",\"Baidu\"],[\"MapR\", \"Blogger\"],[\"Baidu\", \"MapR\"], \\\n",
    "                         [\"Blogger\",\"Google\"], [\"Blogger\", \"Baidu\"],[\"Google\", \"MapR\"]])\n",
    "links = mapLink.groupByKey()\n",
    "\n",
    "ranks = links.map(lambda pairs : (pairs[0],1))\n",
    "print(\"initial ranks:\", ranks.collect())\n",
    "\n",
    "for i in range(5):\n",
    "    cvalues  = links.join(ranks)\n",
    "    contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "    ranks = contribs.reduceByKey(lambda x, y : x+y).mapValues(lambda rank:0.15+0.85*rank)\n",
    "    print(\"ranks(\",i,\"):\", ranks.collect())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.learnbymarketing.com/618/pyspark-rdd-basics-examples/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38\n",
   "language": "python",
   "name": "py38\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
